{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a61c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK wordnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rosamistica\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 'mega_fake_real_political_news.csv' loaded successfully. Rows: 206\n",
      "--- ‚öôÔ∏è Data Preprocessing ---\n",
      "1. Cleaning and processing article titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rosamistica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Rosamistica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Rosamistica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Final processed articles available for modeling: 191\n",
      "3. Data split: Training samples=152, Testing samples=39\n",
      "\n",
      "\n",
      "--- üöÄ Model Training and Evaluation ---\n",
      "1. Fitting TF-IDF Vectorizer...\n",
      "   -> Vocabulary Size (Number of Features): 525\n",
      "2. Training Logistic Regression (Keyword Classifier)...\n",
      "   -> Training Complete in 0.02 seconds.\n",
      "3. Evaluating Model Performance...\n",
      "\n",
      "‚úÖ Model Accuracy on Test Set: 0.9744\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        38\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        39\n",
      "   macro avg       0.49      0.50      0.49        39\n",
      "weighted avg       0.95      0.97      0.96        39\n",
      "\n",
      "--- ‚úÖ Training and Evaluation Complete ---\n",
      "\n",
      "\n",
      "--- üî¨ Interactive Fake News Detector ---\n",
      "Enter a news headline to classify (or type 'quit' to exit).\n",
      "‚ùå Please enter a valid headline.\n",
      "--------------------------------------------------\n",
      "üî¥ CLASSIFICATION: FAKE\n",
      "   Confidence: 95.93%\n",
      "   Keywords Analyzed: fake\n",
      "   Recommendation: üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "üî¥ CLASSIFICATION: FAKE\n",
      "   Confidence: 97.40%\n",
      "   Keywords Analyzed: news\n",
      "   Recommendation: üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "üî¥ CLASSIFICATION: FAKE\n",
      "   Confidence: 97.40%\n",
      "   Keywords Analyzed: news\n",
      "   Recommendation: üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\n",
      "--------------------------------------------------\n",
      "‚ùå Please enter a valid headline.\n",
      "‚ùå Please enter a valid headline.\n",
      "‚ùå Please enter a valid headline.\n",
      "‚ùå Please enter a valid headline.\n",
      "‚ùå Please enter a valid headline.\n",
      "--------------------------------------------------\n",
      "üî¥ CLASSIFICATION: FAKE\n",
      "   Confidence: 97.44%\n",
      "   Keywords Analyzed: viral news\n",
      "   Recommendation: üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\n",
      "--------------------------------------------------\n",
      "‚ùå Please enter a valid headline.\n",
      "--------------------------------------------------\n",
      "üî¥ CLASSIFICATION: FAKE\n",
      "   Confidence: 97.47%\n",
      "   Keywords Analyzed: trump claim invented internet cured world hunger\n",
      "   Recommendation: üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "# --- NLTK Setup ---\n",
    "# Ensure necessary NLTK components are downloaded for robust text processing\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize('test')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK wordnet...\")\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'mega_fake_real_political_news.csv'\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# --- Preprocessing Functions ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing special characters, converting to lowercase,\n",
    "    removing stop words, and lemmatizing the tokens (word base form).\n",
    "    \"\"\"\n",
    "    # Check if input is a valid string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Lowercase and remove punctuation/numbers\n",
    "    text = text.lower()\n",
    "    # Retain only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 2. Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in STOP_WORDS]\n",
    "    \n",
    "    # 3. Lemmatize (crucial for combining similar words like 'running' and 'ran' to 'run')\n",
    "    tokens = [LEMMATIZER.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def train_model(X_train, X_test, y_train, y_test, vectorizer):\n",
    "    \"\"\"\n",
    "    Handles Feature Engineering (TF-IDF) and Model Training (Logistic Regression)\n",
    "    for the keyword-based classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\\n--- üöÄ Model Training and Evaluation ---\")\n",
    "    \n",
    "    # Feature Engineering: TF-IDF\n",
    "    # TF-IDF converts text into numerical feature vectors by weighting words\n",
    "    # based on how frequently they appear in a document relative to the corpus.\n",
    "    # This helps identify the key discriminating \"fake news\" keywords.\n",
    "    print(\"1. Fitting TF-IDF Vectorizer...\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    print(f\"   -> Vocabulary Size (Number of Features): {len(vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    # Classification Model: Logistic Regression\n",
    "    # A simple yet effective linear model often used as a baseline for text classification.\n",
    "    print(\"2. Training Logistic Regression (Keyword Classifier)...\")\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"   -> Training Complete in {training_time:.2f} seconds.\")\n",
    "    \n",
    "    # Predict and Evaluate\n",
    "    print(\"3. Evaluating Model Performance...\")\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    print(\"--- ‚úÖ Training and Evaluation Complete ---\")\n",
    "    \n",
    "    return model, vectorizer\n",
    "\n",
    "def interactive_test(model, vectorizer):\n",
    "    \"\"\"Allows interactive testing of the trained model.\"\"\"\n",
    "    print(\"\\n\\n--- üî¨ Interactive Fake News Detector ---\")\n",
    "    print(\"Enter a news headline to classify (or type 'quit' to exit).\")\n",
    "    \n",
    "    while True:\n",
    "        headline = input(\"\\n> Enter Headline: \")\n",
    "        if headline.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Preprocess the input headline using the same function as training\n",
    "        processed_headline = clean_text(headline)\n",
    "        \n",
    "        if not processed_headline:\n",
    "            print(\"‚ùå Please enter a valid headline.\")\n",
    "            continue\n",
    "            \n",
    "        # Vectorize the processed headline using the fitted vectorizer\n",
    "        headline_vec = vectorizer.transform([processed_headline])\n",
    "        \n",
    "        # Predict the label and confidence\n",
    "        prediction = model.predict(headline_vec)[0]\n",
    "        probabilities = model.predict_proba(headline_vec)[0]\n",
    "        confidence = max(probabilities) * 100\n",
    "        \n",
    "        # Display results\n",
    "        if prediction == 1:\n",
    "            label = \"REAL\"\n",
    "            color = \"üü¢\"\n",
    "            flag_message = \"This article seems to be legitimate.\"\n",
    "        else:\n",
    "            label = \"FAKE\"\n",
    "            color = \"üî¥\"\n",
    "            flag_message = \"üö® POTENTIALLY MISLEADING OR FAKE NEWS DETECTED! (Keyword-based flag)\"\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{color} CLASSIFICATION: {label}\")\n",
    "        print(f\"   Confidence: {confidence:.2f}%\")\n",
    "        print(f\"   Keywords Analyzed: {processed_headline}\")\n",
    "        print(f\"   Recommendation: {flag_message}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Data Loading\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        print(f\"‚úÖ Dataset '{DATA_FILE}' loaded successfully. Rows: {len(df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Dataset file '{DATA_FILE}' not found. Please ensure it's in the same directory.\")\n",
    "        exit()\n",
    "    \n",
    "    # Ensure necessary columns exist (title for text, label for target)\n",
    "    if 'title' not in df.columns or 'label' not in df.columns:\n",
    "        print(\"‚ùå Error: Dataset must contain 'title' and 'label' columns.\")\n",
    "        exit()\n",
    "\n",
    "    # Data Cleaning and Preparation\n",
    "    print(\"--- ‚öôÔ∏è Data Preprocessing ---\")\n",
    "    \n",
    "    # Standardize labels and convert them to numerical format: FAKE=0, REAL=1\n",
    "    df['label'] = df['label'].astype(str).str.upper().replace({'VERIFIED': 'REAL', 'CLAIM': 'FAKE'})\n",
    "    df = df[df['label'].isin(['REAL', 'FAKE'])].copy()\n",
    "    df['target'] = df['label'].apply(lambda x: 1 if x == 'REAL' else 0)\n",
    "\n",
    "    # Apply the cleaning function to the article titles\n",
    "    print(\"1. Cleaning and processing article titles...\")\n",
    "    df['processed_title'] = df['title'].apply(clean_text)\n",
    "    \n",
    "    # Drop rows that resulted in empty processed titles\n",
    "    df.dropna(subset=['processed_title'], inplace=True)\n",
    "    df = df[df['processed_title'].str.len() > 0]\n",
    "    \n",
    "    print(f\"2. Final processed articles available for modeling: {len(df)}\")\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"‚ùå Error: After cleaning, no usable data remains. Check your data.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Split Data into Training and Testing sets (80/20 split)\n",
    "    X = df['processed_title']\n",
    "    y = df['target']\n",
    "    \n",
    "    # stratify=y ensures the ratio of real/fake news is maintained in both sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"3. Data split: Training samples={len(X_train)}, Testing samples={len(X_test)}\")\n",
    "    \n",
    "    # 4. Initialize Vectorizer\n",
    "    # max_features limits the vocabulary size to prevent overfitting and speed up training.\n",
    "    # ngram_range=(1, 2) includes single words (unigrams) and pairs of words (bigrams).\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    \n",
    "    # 5. Train and Test Model\n",
    "    model, vectorizer = train_model(X_train, X_test, y_train, y_test, tfidf_vectorizer)\n",
    "    \n",
    "    # 6. Interactive Demonstration\n",
    "    interactive_test(model, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
